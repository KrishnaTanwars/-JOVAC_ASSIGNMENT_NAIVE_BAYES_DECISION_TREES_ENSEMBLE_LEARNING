{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2bac82f",
   "metadata": {},
   "source": [
    "# Task 1: Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6eba20",
   "metadata": {},
   "source": [
    "Ques.1. What is the core assumption of Naive Bayes?\n",
    "\n",
    "Ans.1. Naive Bayes is based on the fundamental assumption that all features are independent of one another when the class label is known. In other words, knowing the value of one feature doesn’t give any information about the others, which makes calculating probabilities much simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a30266c",
   "metadata": {},
   "source": [
    "Ques.2. Differentiate between GaussianNB, MultinomialNB, and BernoulliNB?\n",
    "\n",
    "Ans.2. \n",
    "GaussianNB is ideal for continuous features and assumes that the data follows a Gaussian (normal) distribution.\n",
    "\n",
    "MultinomialNB is typically used for data represented as counts or frequencies, like term counts in documents.\n",
    "\n",
    "BernoulliNB is suitable for binary feature data, where each feature indicates the presence or absence (1 or 0) of a particular attribute—commonly used in tasks like spam detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ff09a",
   "metadata": {},
   "source": [
    "Ques.3. Why is Naive Bayes considered suitable for high-dimensional data?\n",
    "\n",
    "Ans.3.\n",
    "Naive Bayes performs well with high-dimensional datasets because it treats each feature independently, which simplifies the learning process. This assumption allows the model to compute probabilities for each feature separately, making it fast and effective, especially in applications like text classification where the number of features (e.g., words) can be extremely large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6755fe7",
   "metadata": {},
   "source": [
    "# Task 2: Spam Detection using MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9baaa551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "Confusion Matrix:\n",
      "[[132   0]\n",
      " [  0  16]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krishna\\AppData\\Local\\Temp\\ipykernel_28872\\3000788282.py:9: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['category'] = data['category'].replace({'ham': 0, 'spam': 1})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "data = pd.read_csv(\"email.csv\", sep='\\t', header=None, names=[\"category\", \"text\"])\n",
    "data.dropna(subset=['text'], inplace=True)\n",
    "data['category'] = data['category'].replace({'ham': 0, 'spam': 1})\n",
    "\n",
    "text_train, text_test, label_train, label_test = train_test_split(\n",
    "    data['text'], data['category'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(text_train)\n",
    "test_vectors = vectorizer.transform(text_test)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(train_vectors, label_train)\n",
    "predictions = model.predict(test_vectors)\n",
    "\n",
    "accuracy = accuracy_score(label_test, predictions)\n",
    "precision = precision_score(label_test, predictions)\n",
    "recall = recall_score(label_test, predictions)\n",
    "conf_matrix = confusion_matrix(label_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09779363",
   "metadata": {},
   "source": [
    "# Task 3: GaussianNB with Iris or Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca95e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Gaussian Naive Bayes:\n",
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      1.00      1.00         9\n",
      "   virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Accuracy Comparison Across Models:\n",
      "Naive Bayes Accuracy: 1.0000\n",
      "Logistic Regression Accuracy: 1.0000\n",
      "Decision Tree Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "dataset = load_iris()\n",
    "features, labels = dataset.data, dataset.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_predictions = nb_model.predict(X_test)\n",
    "\n",
    "print(\"Evaluation of Gaussian Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, nb_predictions))\n",
    "print(classification_report(y_test, nb_predictions, target_names=dataset.target_names))\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=200)\n",
    "log_reg.fit(X_train, y_train)\n",
    "lr_predictions = log_reg.predict(X_test)\n",
    "\n",
    "tree_model = DecisionTreeClassifier()\n",
    "tree_model.fit(X_train, y_train)\n",
    "tree_predictions = tree_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy Comparison Across Models:\")\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_score(y_test, nb_predictions):.4f}\")\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, lr_predictions):.4f}\")\n",
    "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, tree_predictions):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
